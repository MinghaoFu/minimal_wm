import torch
import torch.nn as nn
from einops import rearrange, repeat

try:
    from .vit import Transformer, FeedForward
except ImportError:
    # For standalone testing
    from vit import Transformer, FeedForward

class TemporalProjector(nn.Module):
    """
    Temporal Projector with ViT backbone

    è¾“å…¥:
    - z_{t-1}: (b, t, num_patches_z, dim_z) - å‰ä¸€æ—¶åˆ»çš„projected space
    - x_t: (b, t, num_patches_x, dim_x) - å½“å‰æ—¶åˆ»çš„visual embedding

    è¾“å‡º:
    - z_t: (b, t, num_patches_z, dim_z) - å½“å‰æ—¶åˆ»çš„projected space

    æ¶æ„:
    1. åˆ†åˆ«æŠ•å½±z_{t-1}å’Œx_tåˆ°å…±åŒç»´åº¦
    2. ä½¿ç”¨ViT Transformerå¤„ç†è·¨æ¨¡æ€äº¤äº’
    3. è¾“å‡ºæŠ•å½±å›z_tçš„ç›®æ ‡ç»´åº¦
    """

    def __init__(
        self,
        # Standard projector interface (for compatibility)
        in_features=None,   # Will be mapped to x_dim
        out_features=None,  # Will be mapped to output_dim

        # Input dimensions
        num_hist=3,        # å†å²çª—å£å¤§å° (ä½¿ç”¨å‰num_histä¸ªz states)
        z_patches=49,      # z_{t-i} patchæ•°é‡ (7x7)
        z_dim=64,         # z_{t-i} featureç»´åº¦
        x_patches=196,    # x_t patchæ•°é‡ (14x14)
        x_dim=None,       # x_t featureç»´åº¦ (visual + proprio), will use in_features if provided

        # Output dimensions (same as z input)
        output_patches=49,
        output_dim=None,  # will use out_features if provided

        # Temporal processing options
        use_history=True,  # æ˜¯å¦ä½¿ç”¨å†å²ä¿¡æ¯

        # ViT parameters
        hidden_dim=512,   # å…±åŒçš„hiddenç»´åº¦
        depth=4,          # Transformerå±‚æ•°
        heads=8,          # Attentionå¤´æ•°
        mlp_dim=1024,     # MLPéšè—ç»´åº¦
        dim_head=32,      # æ¯ä¸ªheadçš„ç»´åº¦
        dropout=0.1,
        emb_dropout=0.1
    ):
        super().__init__()

        # Parameter mapping for compatibility with standard projector interface
        if in_features is not None:
            x_dim = in_features
        if out_features is not None:
            output_dim = out_features

        # Ensure we have valid dimensions
        assert x_dim is not None, "Must provide either x_dim or in_features"
        assert output_dim is not None, "Must provide either output_dim or out_features"

        self.num_hist = num_hist
        self.z_patches = z_patches
        self.z_dim = z_dim
        self.x_patches = x_patches
        self.x_dim = x_dim
        self.output_patches = output_patches
        self.output_dim = output_dim
        self.hidden_dim = hidden_dim
        self.use_history = use_history

        # ç®€åŒ–ç‰ˆæœ¬ï¼šåªéœ€è¦xçš„æŠ•å½±
        self.x_input_proj = nn.Linear(x_dim, hidden_dim)

        # åªéœ€è¦xçš„ä½ç½®ç¼–ç 
        self.x_pos_embedding = nn.Parameter(torch.randn(1, x_patches, hidden_dim))

        # Dropout
        self.dropout = nn.Dropout(emb_dropout)

        # ViT Transformerå¤„ç†x tokens
        self.transformer = Transformer(
            dim=hidden_dim,
            depth=depth,
            heads=heads,
            dim_head=dim_head,
            mlp_dim=mlp_dim,
            dropout=dropout
        )

        # è¾“å‡ºæŠ•å½±ï¼šä»hidden_dimæŠ•å½±å›ç›®æ ‡ç»´åº¦
        self.output_proj = nn.Sequential(
            nn.LayerNorm(hidden_dim),
            nn.Linear(hidden_dim, output_dim),
            nn.Dropout(dropout)
        )

        # patchæ•°é‡è°ƒæ•´å±‚ï¼šä»x_patcheså‹ç¼©åˆ°output_patches
        if x_patches != output_patches:
            self.patch_adjust = PatchAdjustment(
                input_patches=x_patches,  # 196 patches from x_t
                output_patches=output_patches,  # 49 patches for z_t
                dim=output_dim
            )
        else:
            self.patch_adjust = nn.Identity()

    def forward(self, x_sequence, z_history=None):
        """
        Temporal projector with cumulative historical z states

        Logic: z_t = f(x_t, z_{t-3}, z_{t-2}, z_{t-1})
        - z1 â† f(x1, 0, 0, 0) - pad with zeros
        - z2 â† f(x2, z1, 0, 0) - pad with zeros
        - z3 â† f(x3, z1, z2, 0) - pad with zeros
        - z4 â† f(x4, z1, z2, z3) - full history

        Args:
            x_sequence: (b, t, x_patches, x_dim) - visual+proprio embedding sequence
            z_history: Not used - we maintain history internally

        Returns:
            z_sequence: (b, t, output_patches, output_dim) - projected sequence
        """
        b, t, x_patches, x_dim = x_sequence.shape
        z_outputs = []
        z_states = []  # Store all computed z states

        for i in range(t):
            x_t = x_sequence[:, i]  # (b, x_patches, x_dim)

            if self.use_history:
                # Build history for current timestep
                if i == 0:
                    # z1: use 3 zero padding
                    z_hist = torch.zeros(b, self.num_hist, self.output_patches, self.output_dim,
                                       device=x_sequence.device, dtype=x_sequence.dtype)
                elif i == 1:
                    # z2: use z1 + 2 zero padding
                    z_hist = torch.cat([
                        z_states[0].unsqueeze(1),  # z1
                        torch.zeros(b, 2, self.output_patches, self.output_dim,
                                  device=x_sequence.device, dtype=x_sequence.dtype)
                    ], dim=1)
                elif i == 2:
                    # z3: use z1, z2 + 1 zero padding
                    z_hist = torch.cat([
                        torch.stack(z_states[:2], dim=1),  # z1, z2
                        torch.zeros(b, 1, self.output_patches, self.output_dim,
                                  device=x_sequence.device, dtype=x_sequence.dtype)
                    ], dim=1)
                else:
                    # z4+: use last 3 z states
                    z_hist = torch.stack(z_states[i-3:i], dim=1)  # z_{t-3}, z_{t-2}, z_{t-1}

                # TODO: Process x_t with history context using cross-modal attention
                # For now, just process x_t independently
                x_proj = self.x_input_proj(x_t)  # (b, x_patches, hidden_dim)
            else:
                # Simple mode: just process x_t without history
                x_proj = self.x_input_proj(x_t)  # (b, x_patches, hidden_dim)

            x_proj = x_proj + self.x_pos_embedding
            x_proj = self.dropout(x_proj)

            # ViT processing
            x_proj = self.transformer(x_proj)  # (b, x_patches, hidden_dim)

            # Output projection and patch adjustment
            z_t = self.output_proj(x_proj)  # (b, x_patches, output_dim)
            z_t = self.patch_adjust(z_t)     # (b, output_patches, output_dim)

            z_outputs.append(z_t)
            if self.use_history:
                z_states.append(z_t)  # Store for future history only if using history

        # Stack outputs
        z_sequence = torch.stack(z_outputs, dim=1)  # (b, t, output_patches, output_dim)
        return z_sequence


class CrossModalAttention(nn.Module):
    """è·¨æ¨¡æ€æ³¨æ„åŠ›ï¼šz tokensä½œä¸ºqueryï¼Œx tokensä½œä¸ºkey/value"""

    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):
        super().__init__()
        inner_dim = dim_head * heads
        project_out = not (heads == 1 and dim_head == dim)

        self.heads = heads
        self.scale = dim_head ** -0.5

        self.norm_q = nn.LayerNorm(dim)
        self.norm_kv = nn.LayerNorm(dim)

        self.attend = nn.Softmax(dim=-1)
        self.dropout = nn.Dropout(dropout)

        self.to_q = nn.Linear(dim, inner_dim, bias=False)
        self.to_k = nn.Linear(dim, inner_dim, bias=False)
        self.to_v = nn.Linear(dim, inner_dim, bias=False)

        self.to_out = nn.Sequential(
            nn.Linear(inner_dim, dim),
            nn.Dropout(dropout)
        ) if project_out else nn.Identity()

    def forward(self, z_tokens, x_tokens):
        """
        Args:
            z_tokens: (b, z_patches, dim) - query
            x_tokens: (b, x_patches, dim) - key, value
        """
        z_norm = self.norm_q(z_tokens)
        x_norm = self.norm_kv(x_tokens)

        # Generate Q from z tokens, K,V from x tokens
        q = self.to_q(z_norm)
        k = self.to_k(x_norm)
        v = self.to_v(x_norm)

        # Reshape for multi-head attention
        q = rearrange(q, 'b n (h d) -> b h n d', h=self.heads)
        k = rearrange(k, 'b n (h d) -> b h n d', h=self.heads)
        v = rearrange(v, 'b n (h d) -> b h n d', h=self.heads)

        # Attention
        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale
        attn = self.attend(dots)
        attn = self.dropout(attn)

        out = torch.matmul(attn, v)
        out = rearrange(out, 'b h n d -> b n (h d)')

        # Residual connection
        return z_tokens + self.to_out(out)


class PatchAdjustment(nn.Module):
    """è°ƒæ•´patchæ•°é‡çš„å±‚"""

    def __init__(self, input_patches, output_patches, dim):
        super().__init__()
        self.input_patches = input_patches
        self.output_patches = output_patches

        if input_patches == output_patches:
            self.adjust = nn.Identity()
        elif input_patches < output_patches:
            # ä¸Šé‡‡æ · - ä½¿ç”¨å­¦ä¹ çš„æ’å€¼
            self.adjust = nn.Sequential(
                nn.Linear(dim, dim * 2),
                nn.GELU(),
                nn.Linear(dim * 2, dim),
                nn.LayerNorm(dim)
            )
            # å­¦ä¹ æ’å€¼æƒé‡
            self.interpolation = nn.Parameter(torch.randn(output_patches, input_patches))
        else:
            # ä¸‹é‡‡æ · - ä½¿ç”¨æ³¨æ„åŠ›æ± åŒ–
            self.adjust = nn.MultiheadAttention(
                embed_dim=dim,
                num_heads=8,
                batch_first=True
            )
            self.queries = nn.Parameter(torch.randn(1, output_patches, dim))

    def forward(self, x):
        """
        Args:
            x: (b, input_patches, dim)
        Returns:
            out: (b, output_patches, dim)
        """
        if self.input_patches == self.output_patches:
            return self.adjust(x)
        elif self.input_patches < self.output_patches:
            # ä¸Šé‡‡æ ·
            b = x.shape[0]
            # çº¿æ€§æ’å€¼
            weights = torch.softmax(self.interpolation, dim=-1)  # (output_patches, input_patches)
            x_upsampled = torch.matmul(weights, x)  # (b, output_patches, dim)
            return self.adjust(x_upsampled)
        else:
            # ä¸‹é‡‡æ ·ä½¿ç”¨æ³¨æ„åŠ›
            b = x.shape[0]
            queries = self.queries.repeat(b, 1, 1)  # (b, output_patches, dim)
            out, _ = self.adjust(queries, x, x)  # (b, output_patches, dim)
            return out


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("ğŸ§ª Testing TemporalProjector...")

    # å‚æ•°è®¾ç½®
    batch_size = 2
    time_steps = 3
    z_patches = 49  # 7x7
    z_dim = 64
    x_patches = 196  # 14x14
    x_dim = 394  # visual + proprio

    # åˆ›å»ºæ¨¡å‹
    projector = TemporalProjector(
        z_patches=z_patches,
        z_dim=z_dim,
        x_patches=x_patches,
        x_dim=x_dim,
        output_patches=z_patches,
        output_dim=z_dim,
        hidden_dim=512,
        depth=4,
        heads=8,
        mlp_dim=1024
    )

    print(f"ğŸ“Š Model parameters: {sum(p.numel() for p in projector.parameters()):,}")

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    x_sequence = torch.randn(batch_size, time_steps, x_patches, x_dim)

    print(f"ğŸ“¥ Input shapes:")
    print(f"   x_sequence: {x_sequence.shape}")

    # Forward pass
    try:
        z_sequence = projector(x_sequence)
        print(f"âœ… Output shape: {z_sequence.shape}")
        assert z_sequence.shape == (batch_size, time_steps, z_patches, z_dim)
        print("âœ… Shape test passed!")

        # æµ‹è¯•æ¢¯åº¦
        loss = z_sequence.sum()
        loss.backward()
        print("âœ… Gradient test passed!")

    except Exception as e:
        print(f"âŒ Test failed: {e}")
        raise

    print("ğŸ‰ All tests passed!")