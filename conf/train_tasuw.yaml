defaults:
  - _self_
  - env: wall
  - encoder: dino
  - action_encoder: proprio
  - proprio_encoder: proprio
  - decoder: vqvae
  - predictor: vit
  - projector: linear_temporal # linear vae
  - override hydra/launcher: basic

ckpt_base_path: ./outputs_${env.name}
resume_folder: none #/data2/minghao/wm/outputs_wall/2025-10-03/04-35-04
val_only: false

latent_export:
  enabled: false
  output_path: null
  splits: ["train"]
  rgb_key: agentview_image
  latent_grid_size: 14
  mode: trajectory # slice

encoder:
  _target_: models.dino.DinoV2Encoder
  name: "dinov2_vits14"
  feature_key: "x_norm_patchtokens"

# Base path for local execution


# Clean hydra config without any submitit
hydra:
  run:
    dir: ${ckpt_base_path}/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: ${ckpt_base_path}/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}

# Single GPU training settings
training:
  seed: 0
  epochs: 200  # Full training
  batch_size: 32  # Single GPU batch size
  save_every_x_epoch: 1
  reconstruct_every_x_batch: 500
  num_reconstruct_samples: 2
  encoder_lr: 1e-6
  decoder_lr: 3e-4
  predictor_lr: 5e-4
  action_encoder_lr: 5e-4

img_size: 224 # should be a multiple of 224
frameskip: 5
concat_dim: 1

normalize_action: True

num_hist: 3
num_pred: 1 # only supports 1
has_predictor: True # set this to False for only training a decoder
has_decoder: True # Keep decoder - uses original 384D DINO features like original DINO-WM

encoder_emb_dim: 384
# action encoder - 适配 robomimic 7D 动作空间 (6关节+夹爪)
action_emb_dim: 16  # 7D -> 16D embedding
num_action_repeat: 1

# proprio encoder - 适配 robomimic 16D proprio空间 (机器人状态+速度)
proprio_emb_dim: 64
num_proprio_repeat: 1

# Temporal projector with patch compression configuration
# Input: x_t (416 dim, 196 patches) + z_{t-1} history
# Output: z_t (64 dim, 49 patches compressed)
projected_dim: 64  # Target compressed dimension
compressed_patches: 49  # Target compressed patch count (7x7)
original_patches: 196   # Original patch count (14x14)
alignment_dim: 16  # InfoNCE alignment with proprio dimension

model:
  _target_: models.visual_world_model.VWorldModel
  image_size: ${img_size}
  num_hist: ${num_hist}
  num_pred: ${num_pred}
  train_encoder: False
  alignment_dim: ${alignment_dim}
  train_predictor: True
  train_decoder: True
  projected_dim: ${projected_dim}

# Alignment settings with new architecture: 12D z-space, InfoNCE alignment, full dynamics
alignment:
  state_consistency_loss_weight: 1.0
  alignment_regularization: 0.01
  latent_dynamics_loss_weight: 1.0  # Full 64D latent dynamics prediction (t-1 → t)
  # Note: We predict dynamics in the full 64D mixed latent space after projection

# DINO reconstruction is not used - removed

debug: False 

# Auto GPU selection
num_gpus: 1  # Number of GPUs to automatically select
min_free_memory_gb: 2.0  # Minimum free memory required per GPU

# Planning params for planning eval jobs launched during training
plan_settings: 
  # plan_cfg_path: conf/plan.yaml # set to null for no planning evals
  plan_cfg_path: null 
  planner: ['gd', 'cem']
  goal_source: ['dset', 'random_state']
  goal_H: [5]
  alpha: [0.1, 1]

# WandB settings
wandb:
  project: tasuw
  name: "linear_projector_history_196_64"
  entity: minghao_workaholic
